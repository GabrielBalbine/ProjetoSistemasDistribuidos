# Sistema de Chat Distribu√≠do

Este √© um projeto de um sistema de chat em tempo real, polyglot (Python, Go, JavaScript), constru√≠do sobre uma arquitetura de microsservi√ßos tolerante a falhas. A aplica√ß√£o utiliza ZeroMQ, Docker Compose e implementa conceitos avan√ßados de sistemas distribu√≠dos, como elei√ß√£o de coordenador, replica√ß√£o de estado e consist√™ncia.

## ‚ú® Features

* **Alta Disponibilidade:** Utiliza m√∫ltiplas r√©plicas do servidor e um algoritmo de elei√ß√£o de Coordenador (L√≠der) para garantir que o sistema continue funcionando mesmo se o l√≠der atual falhar.
* **Replica√ß√£o de Estado:** O estado do sistema (usu√°rios, canais) √© replicado em um volume compartilhado (`election-data`), permitindo que um novo l√≠der assuma o trabalho instantaneamente sem perda de dados.
* **Arquitetura Polyglot:** Demonstra a interoperabilidade entre diferentes linguagens:
    * **Servidor (Python):** O c√©rebro da aplica√ß√£o, com a l√≥gica de neg√≥cio e elei√ß√£o.
    * **Cliente Interativo (JavaScript):** Interface de linha de comando para usu√°rios.
    * **Bot Automatizado (Go):** Cliente n√£o-humano que gera tr√°fego e √© resiliente a falhas.
* **Seguran√ßa:** Implementa cadastro e login de usu√°rios com hashing de senhas (`SHA256`) e sess√µes **stateless** (sem estado) via **JSON Web Tokens (JWT)**, permitindo que a autentica√ß√£o funcione perfeitamente mesmo ap√≥s a troca de l√≠der.
* **Comunica√ß√£o com ZeroMQ:** Utiliza dois padr√µes de comunica√ß√£o distintos:
    * **Request-Reply:** Para comandos e opera√ß√µes s√≠ncronas.
    * **Publisher-Subscriber:** Para a distribui√ß√£o de mensagens em tempo real.
* **Conceitos Implementados:**
    * **Rel√≥gios L√≥gicos (Lamport):** Cada requisi√ß√£o e resposta incrementa e sincroniza um contador de Lamport, garantindo a ordem causal dos eventos.
    * **Sincroniza√ß√£o de Rel√≥gio (Berkeley):** O L√≠der atua como um Coordenador de tempo (`getTime`), permitindo que os clientes comparem seu rel√≥gio com o do servidor.
    * **Elei√ß√£o de Coordenador:** Implementa o **Algoritmo do Valent√£o** adaptado para um mecanismo robusto de **Lock File** em um volume compartilhado, prevenindo "Split-Brain".
    * **Sincroniza√ß√£o de Dados (Replica√ß√£o):** O L√≠der escreve todas as altera√ß√µes de estado (novos usu√°rios, novos canais) no volume compartilhado. Um novo l√≠der, ao ser eleito, l√™ esses arquivos e carrega o estado, garantindo a consist√™ncia e a replica√ß√£o dos dados.

## üìê Arquitetura

O sistema √© orquestrado pelo Docker Compose e se baseia em um cluster de servidores com um √∫nico l√≠der ativo.

1.  **Cluster de Servidores:** M√∫ltiplas inst√¢ncias (`servidor-1`, `2`, `3`) competem pela lideran√ßa. Apenas o **L√≠der** se conecta aos brokers de trabalho e processa as requisi√ß√µes. Os **Seguidores** monitoram o l√≠der.
2.  **Volume Compartilhado (`election-data`):** Atua como a fonte √∫nica da verdade para a elei√ß√£o (atrav√©s do `leader.lock`) e para o estado do sistema (`usuarios.json`, `canais.json`, `messages.log`).
3.  **Brokers ZeroMQ:** Dois brokers desacoplam a comunica√ß√£o: um `ROUTER/DEALER` para comandos e um `XPUB/XSUB` para mensagens em tempo real.
4.  **Clientes (JavaScript e Go):** Clientes resilientes que sabem lidar com a falha tempor√°ria do l√≠der, usando um `timeout` e tentando novamente.

```mermaid
graph TD
    %% --- Clientes ---
    subgraph Clientes
        A[Cliente Interativo JavaScript]
        B[Bot Automatizado Go]
    end

    %% --- Infraestrutura ---
    subgraph Infraestrutura_de_Mensagens_Docker
        C[Broker REQ/REP]
        D[Proxy PUB/SUB]
    end
    
    %% --- Servidores ---
    subgraph Cluster_de_Servidores_Docker
        S1[Servidor 1 Seguidor]
        S2[Servidor 2 Seguidor]
        SL[Servidor 3 L√≠der]
    end
    
    %% --- Volume ---
    subgraph Estado_Persistente_Docker_Volume
        V[(election-data)]
        V -->|leader.lock| SL
        V -->|usuarios.json, canais.json| SL
        S1 -->|Le leader.lock| V
        S2 -->|Le leader.lock| V
    end

    %% --- Fluxo principal ---
    A -->|Comandos REQ| C
    B -->|Comandos REQ| C
    C -->|Encaminha para o L√≠der| SL
    SL -->|Responde REP| C
    C -->|Retorna para Cliente| A
    C -->|Retorna para Bot| B
    
    SL -->|Publica Mensagens PUB| D
    D -->|Distribui SUB| A
    D -->|Distribui SUB| B



```

## üõ†Ô∏è Pr√©-requisitos

* [Docker](https://www.docker.com/get-started) e Docker Compose
* [Go](https://go.dev/doc/install) (para gerar os arquivos `go.mod`/`go.sum` na primeira vez)

## üöÄ Como Rodar

1.  **Primeira Vez (Setup do Go):** Se for a primeira vez rodando o projeto, entre na pasta `proxy/` e prepare o m√≥dulo do Go:
    ```bash
    cd proxy
    go mod init bot-go
    go mod tidy
    cd ..
    ```

2.  **Limpeza (Opcional, mas Recomendado):** Para come√ßar de um estado 100% limpo, na pasta raiz:
    ```bash
    docker compose down
    docker volume rm projetosistemasdistribuidos_election-data
    ```

3.  **Suba a Orquestra:** Na pasta raiz, construa e inicie todos os servi√ßos. Deixe este terminal aberto para observar os logs.
    ```bash
    docker compose up --build
    ```
    *Observe a elei√ß√£o acontecer! Apenas um servidor se tornar√° o L√çDER.*

4.  **Execute o Cliente Interativo:** Abra um **novo terminal** e rode o cliente em seu container:
    ```bash
    docker compose run --rm cliente-js
    ```
    *Siga as instru√ß√µes para se cadastrar e fazer login.*

## üî¨ Como Testar a Alta Disponibilidade

1.  Com o sistema rodando, use o cliente para criar um usu√°rio (`userA`) e um canal (`canal-teste`).
2.  Nos logs do `docker compose`, identifique qual servidor √© o L√çDER (ex: `servidor-3-1`).
3.  Abra um **terceiro terminal** e derrube o l√≠der de prop√≥sito:
    ```bash
    # Substitua pelo nome correto do container do seu l√≠der
    docker stop projetosistemasdistribuidos-servidor-3-1 
    ```
4.  **Observe a M√°gica:** Nos logs principais, voc√™ ver√° os seguidores detectarem a falha e um novo l√≠der ser eleito.
5.  **Teste a Resili√™ncia:** No terminal do cliente, o primeiro comando (ex: `listar_canais`) pode dar timeout. Tente o mesmo comando novamente.
6.  **Teste a Replica√ß√£o:** O comando `listar_canais` **deve funcionar** e **mostrar o `canal-teste`** que voc√™ criou com o l√≠der antigo. Tente fazer login com o `userA` em outro cliente: **deve funcionar**, provando que os dados foram 100% replicados.

---
Criado por: **Gabriel Balbine** - Novembro de 2025
